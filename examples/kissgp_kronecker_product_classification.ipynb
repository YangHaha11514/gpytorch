{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "n = 30\n",
    "train_x = torch.zeros(int(pow(n, 2)), 2)\n",
    "train_y = torch.zeros(int(pow(n, 2)))\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        train_x[i * n + j][0] = float(i) / (n - 1)\n",
    "        train_x[i * n + j][1] = float(j) / (n - 1)\n",
    "        train_y[i * n + j] = pow(-1, int(2 * i / n + int( 2 * j / n)))\n",
    "train_x = Variable(train_x)\n",
    "train_y = Variable(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "from gpytorch.kernels import RBFKernel, GridInterpolationKernel\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.likelihoods import GaussianLikelihood, BernoulliLikelihood\n",
    "from gpytorch.random_variables import GaussianRandomVariable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GPClassificationModel(gpytorch.GPModel):\n",
    "    def __init__(self):\n",
    "        super(GPClassificationModel,self).__init__(BernoulliLikelihood())\n",
    "        self.mean_module = ConstantMean()\n",
    "        self.covar_module = RBFKernel(log_lengthscale_bounds=(-6, 6))\n",
    "        self.grid_covar_module = GridInterpolationKernel(self.covar_module)\n",
    "        self.register_parameter('log_outputscale', nn.Parameter(torch.Tensor([0])), bounds=(-6,6))\n",
    "        self.initialize_interpolation_grid(10, [(0, 1), (0, 1)])\n",
    "    \n",
    "    def forward(self,x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.grid_covar_module(x)\n",
    "        covar_x = covar_x.mul(self.log_outputscale.exp())\n",
    "        latent_pred = GaussianRandomVariable(mean_x, covar_x)\n",
    "        return latent_pred\n",
    "\n",
    "prior_model = GPClassificationModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_model_and_predictions(model, plot_train_data=True):\n",
    "    f, observed_ax = plt.subplots(1, 1, figsize=(4, 3))\n",
    "    n = 100\n",
    "    test_x = Variable(torch.zeros(int(pow(n, 2)), 2))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            test_x.data[i * n + j][0] = float(i) / (n-1)\n",
    "            test_x.data[i * n + j][1] = float(j) / (n-1)\n",
    "    observed_pred = model(test_x)\n",
    "\n",
    "    def ax_plot(ax, rand_var, title):\n",
    "        pred_labels = rand_var.mean().ge(0.5).float().mul(2).sub(1).data.numpy()\n",
    "        print rand_var.mean().ge(0.5).float().mul(2).sub(1).view(n, n)\n",
    "        color = []\n",
    "        for i in range(len(pred_labels)):\n",
    "            if pred_labels[i] == 1:\n",
    "                color.append('b')\n",
    "            else:\n",
    "                color.append('r')\n",
    "        ax.scatter(test_x.data[:, 0].numpy(), test_x.data[:, 1].numpy(), color=color)\n",
    "        ax.set_ylim([-0.5, 1.5])\n",
    "        ax.set_title(title)\n",
    "    \n",
    "    ax_plot(observed_ax, observed_pred, 'Observed Values (Likelihood)')\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruihanwu/playground/gpytorch/gpytorch/kernels/rbf_kernel.py:42: UserWarning: other is not broadcastable to self, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n",
      "  grad.mul_(grad_output.transpose(0, 1))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/20 - Loss: 318447.438   log_lengthscale: 0.000\n",
      "Iter 2/20 - Loss: 61300.785   log_lengthscale: -0.100\n",
      "Iter 3/20 - Loss: 34305.590   log_lengthscale: -0.183\n",
      "Iter 4/20 - Loss: 38543.992   log_lengthscale: -0.262\n",
      "Iter 5/20 - Loss: 25560.086   log_lengthscale: -0.345\n",
      "Iter 6/20 - Loss: 23333.410   log_lengthscale: -0.431\n",
      "Iter 7/20 - Loss: 19702.008   log_lengthscale: -0.514\n",
      "Iter 8/20 - Loss: 21653.717   log_lengthscale: -0.601\n",
      "Iter 9/20 - Loss: 19578.395   log_lengthscale: -0.685\n",
      "Iter 10/20 - Loss: 23175.676   log_lengthscale: -0.768\n",
      "Iter 11/20 - Loss: 12649.883   log_lengthscale: -0.850\n",
      "Iter 12/20 - Loss: 19839.990   log_lengthscale: -0.932\n",
      "Iter 13/20 - Loss: 19825.902   log_lengthscale: -1.016\n",
      "Iter 14/20 - Loss: 14038.142   log_lengthscale: -1.100\n",
      "Iter 15/20 - Loss: 13634.162   log_lengthscale: -1.186\n",
      "Iter 16/20 - Loss: 11585.962   log_lengthscale: -1.269\n",
      "Iter 17/20 - Loss: 13304.151   log_lengthscale: -1.349\n",
      "Iter 18/20 - Loss: 15788.870   log_lengthscale: -1.425\n",
      "Iter 19/20 - Loss: 9171.696   log_lengthscale: -1.500\n",
      "Iter 20/20 - Loss: 8438.670   log_lengthscale: -1.575\n",
      "Iter 21/20 - Loss: 8360.691   log_lengthscale: -1.647\n",
      "Iter 22/20 - Loss: 8547.058   log_lengthscale: -1.717\n",
      "Iter 23/20 - Loss: 6599.546   log_lengthscale: -1.785\n",
      "Iter 24/20 - Loss: 7188.177   log_lengthscale: -1.851\n",
      "Iter 25/20 - Loss: 6952.015   log_lengthscale: -1.915\n",
      "Iter 26/20 - Loss: 7085.743   log_lengthscale: -1.974\n",
      "Iter 27/20 - Loss: 3879.658   log_lengthscale: -2.030\n",
      "Iter 28/20 - Loss: 4353.373   log_lengthscale: -2.084\n",
      "Iter 29/20 - Loss: 4280.243   log_lengthscale: -2.136\n",
      "Iter 30/20 - Loss: 4391.777   log_lengthscale: -2.185\n",
      "Iter 31/20 - Loss: 4901.996   log_lengthscale: -2.231\n",
      "Iter 32/20 - Loss: 4923.396   log_lengthscale: -2.275\n",
      "Iter 33/20 - Loss: 4572.158   log_lengthscale: -2.317\n",
      "Iter 34/20 - Loss: 4825.515   log_lengthscale: -2.357\n",
      "Iter 35/20 - Loss: 4606.146   log_lengthscale: -2.396\n",
      "Iter 36/20 - Loss: 5301.470   log_lengthscale: -2.433\n",
      "Iter 37/20 - Loss: 3509.680   log_lengthscale: -2.469\n",
      "Iter 38/20 - Loss: 4372.932   log_lengthscale: -2.504\n",
      "Iter 39/20 - Loss: 3094.010   log_lengthscale: -2.537\n",
      "Iter 40/20 - Loss: 3916.500   log_lengthscale: -2.569\n",
      "Iter 41/20 - Loss: 4638.827   log_lengthscale: -2.600\n",
      "Iter 42/20 - Loss: 4120.408   log_lengthscale: -2.628\n",
      "Iter 43/20 - Loss: 3221.718   log_lengthscale: -2.656\n",
      "Iter 44/20 - Loss: 2157.284   log_lengthscale: -2.683\n",
      "Iter 45/20 - Loss: 3044.392   log_lengthscale: -2.708\n",
      "Iter 46/20 - Loss: 2808.060   log_lengthscale: -2.733\n",
      "Iter 47/20 - Loss: 2600.176   log_lengthscale: -2.756\n",
      "Iter 48/20 - Loss: 2723.725   log_lengthscale: -2.777\n",
      "Iter 49/20 - Loss: 2564.767   log_lengthscale: -2.798\n",
      "Iter 50/20 - Loss: 2708.289   log_lengthscale: -2.817\n",
      "Iter 51/20 - Loss: 2273.637   log_lengthscale: -2.836\n",
      "Iter 52/20 - Loss: 2370.623   log_lengthscale: -2.855\n",
      "Iter 53/20 - Loss: 2166.129   log_lengthscale: -2.872\n",
      "Iter 54/20 - Loss: 1988.505   log_lengthscale: -2.889\n",
      "Iter 55/20 - Loss: 2561.358   log_lengthscale: -2.905\n",
      "Iter 56/20 - Loss: 2123.151   log_lengthscale: -2.920\n",
      "Iter 57/20 - Loss: 2331.333   log_lengthscale: -2.935\n",
      "Iter 58/20 - Loss: 2149.931   log_lengthscale: -2.949\n",
      "Iter 59/20 - Loss: 2037.077   log_lengthscale: -2.963\n",
      "Iter 60/20 - Loss: 2157.674   log_lengthscale: -2.976\n",
      "Iter 61/20 - Loss: 1754.306   log_lengthscale: -2.989\n",
      "Iter 62/20 - Loss: 1914.898   log_lengthscale: -3.001\n",
      "Iter 63/20 - Loss: 2054.681   log_lengthscale: -3.014\n",
      "Iter 64/20 - Loss: 2207.042   log_lengthscale: -3.025\n",
      "Iter 65/20 - Loss: 2045.600   log_lengthscale: -3.036\n",
      "Iter 66/20 - Loss: 1833.959   log_lengthscale: -3.047\n",
      "Iter 67/20 - Loss: 1889.367   log_lengthscale: -3.057\n",
      "Iter 68/20 - Loss: 1537.931   log_lengthscale: -3.067\n",
      "Iter 69/20 - Loss: 1851.292   log_lengthscale: -3.076\n",
      "Iter 70/20 - Loss: 1282.407   log_lengthscale: -3.084\n",
      "Iter 71/20 - Loss: 1647.698   log_lengthscale: -3.092\n",
      "Iter 72/20 - Loss: 1527.063   log_lengthscale: -3.100\n",
      "Iter 73/20 - Loss: 1422.397   log_lengthscale: -3.107\n",
      "Iter 74/20 - Loss: 1525.040   log_lengthscale: -3.113\n",
      "Iter 75/20 - Loss: 1214.831   log_lengthscale: -3.120\n",
      "Iter 76/20 - Loss: 1587.614   log_lengthscale: -3.127\n",
      "Iter 77/20 - Loss: 1367.115   log_lengthscale: -3.133\n",
      "Iter 78/20 - Loss: 1297.485   log_lengthscale: -3.140\n",
      "Iter 79/20 - Loss: 1258.271   log_lengthscale: -3.146\n",
      "Iter 80/20 - Loss: 1327.916   log_lengthscale: -3.152\n",
      "Iter 81/20 - Loss: 1580.956   log_lengthscale: -3.157\n",
      "Iter 82/20 - Loss: 996.015   log_lengthscale: -3.163\n",
      "Iter 83/20 - Loss: 1267.477   log_lengthscale: -3.168\n",
      "Iter 84/20 - Loss: 1205.260   log_lengthscale: -3.173\n",
      "Iter 85/20 - Loss: 1037.584   log_lengthscale: -3.178\n",
      "Iter 86/20 - Loss: 1092.734   log_lengthscale: -3.183\n",
      "Iter 87/20 - Loss: 1052.106   log_lengthscale: -3.187\n",
      "Iter 88/20 - Loss: 1129.082   log_lengthscale: -3.192\n",
      "Iter 89/20 - Loss: 1174.583   log_lengthscale: -3.196\n",
      "Iter 90/20 - Loss: 1085.442   log_lengthscale: -3.200\n",
      "Iter 91/20 - Loss: 981.030   log_lengthscale: -3.205\n",
      "Iter 92/20 - Loss: 984.640   log_lengthscale: -3.209\n",
      "Iter 93/20 - Loss: 916.232   log_lengthscale: -3.212\n",
      "Iter 94/20 - Loss: 1121.702   log_lengthscale: -3.216\n",
      "Iter 95/20 - Loss: 1362.990   log_lengthscale: -3.220\n",
      "Iter 96/20 - Loss: 1084.406   log_lengthscale: -3.223\n",
      "Iter 97/20 - Loss: 1076.988   log_lengthscale: -3.227\n",
      "Iter 98/20 - Loss: 1288.899   log_lengthscale: -3.230\n",
      "Iter 99/20 - Loss: 1092.896   log_lengthscale: -3.234\n",
      "Iter 100/20 - Loss: 1122.788   log_lengthscale: -3.237\n",
      "Iter 101/20 - Loss: 820.772   log_lengthscale: -3.240\n",
      "Iter 102/20 - Loss: 1021.942   log_lengthscale: -3.243\n",
      "Iter 103/20 - Loss: 1245.580   log_lengthscale: -3.246\n",
      "Iter 104/20 - Loss: 895.364   log_lengthscale: -3.249\n",
      "Iter 105/20 - Loss: 1020.686   log_lengthscale: -3.252\n",
      "Iter 106/20 - Loss: 1016.171   log_lengthscale: -3.255\n",
      "Iter 107/20 - Loss: 884.594   log_lengthscale: -3.258\n",
      "Iter 108/20 - Loss: 850.872   log_lengthscale: -3.260\n",
      "Iter 109/20 - Loss: 963.641   log_lengthscale: -3.263\n",
      "Iter 110/20 - Loss: 932.756   log_lengthscale: -3.265\n",
      "Iter 111/20 - Loss: 850.218   log_lengthscale: -3.267\n",
      "Iter 112/20 - Loss: 992.824   log_lengthscale: -3.270\n",
      "Iter 113/20 - Loss: 862.940   log_lengthscale: -3.272\n",
      "Iter 114/20 - Loss: 716.508   log_lengthscale: -3.274\n",
      "Iter 115/20 - Loss: 837.402   log_lengthscale: -3.276\n",
      "Iter 116/20 - Loss: 680.421   log_lengthscale: -3.278\n",
      "Iter 117/20 - Loss: 925.542   log_lengthscale: -3.280\n",
      "Iter 118/20 - Loss: 846.969   log_lengthscale: -3.283\n",
      "Iter 119/20 - Loss: 856.871   log_lengthscale: -3.285\n",
      "Iter 120/20 - Loss: 851.230   log_lengthscale: -3.287\n",
      "Iter 121/20 - Loss: 804.410   log_lengthscale: -3.288\n",
      "Iter 122/20 - Loss: 862.051   log_lengthscale: -3.290\n",
      "Iter 123/20 - Loss: 825.762   log_lengthscale: -3.292\n",
      "Iter 124/20 - Loss: 976.877   log_lengthscale: -3.294\n",
      "Iter 125/20 - Loss: 871.697   log_lengthscale: -3.296\n",
      "Iter 126/20 - Loss: 908.485   log_lengthscale: -3.297\n",
      "Iter 127/20 - Loss: 787.771   log_lengthscale: -3.299\n",
      "Iter 128/20 - Loss: 991.489   log_lengthscale: -3.301\n",
      "Iter 129/20 - Loss: 1056.009   log_lengthscale: -3.303\n",
      "Iter 130/20 - Loss: 1065.467   log_lengthscale: -3.305\n",
      "Iter 131/20 - Loss: 1211.141   log_lengthscale: -3.306\n",
      "Iter 132/20 - Loss: 1019.719   log_lengthscale: -3.308\n",
      "Iter 133/20 - Loss: 1091.797   log_lengthscale: -3.311\n",
      "Iter 134/20 - Loss: 891.398   log_lengthscale: -3.313\n",
      "Iter 135/20 - Loss: 959.122   log_lengthscale: -3.316\n",
      "Iter 136/20 - Loss: 882.862   log_lengthscale: -3.319\n",
      "Iter 137/20 - Loss: 863.290   log_lengthscale: -3.321\n",
      "Iter 138/20 - Loss: 984.228   log_lengthscale: -3.324\n",
      "Iter 139/20 - Loss: 844.711   log_lengthscale: -3.327\n",
      "Iter 140/20 - Loss: 990.900   log_lengthscale: -3.329\n",
      "Iter 141/20 - Loss: 891.956   log_lengthscale: -3.332\n",
      "Iter 142/20 - Loss: 799.737   log_lengthscale: -3.334\n",
      "Iter 143/20 - Loss: 886.465   log_lengthscale: -3.336\n",
      "Iter 144/20 - Loss: 912.978   log_lengthscale: -3.339\n",
      "Iter 145/20 - Loss: 759.850   log_lengthscale: -3.341\n",
      "Iter 146/20 - Loss: 641.389   log_lengthscale: -3.343\n",
      "Iter 147/20 - Loss: 744.527   log_lengthscale: -3.345\n",
      "Iter 148/20 - Loss: 952.097   log_lengthscale: -3.348\n",
      "Iter 149/20 - Loss: 736.889   log_lengthscale: -3.350\n",
      "Iter 150/20 - Loss: 693.549   log_lengthscale: -3.352\n",
      "Iter 151/20 - Loss: 717.991   log_lengthscale: -3.354\n",
      "Iter 152/20 - Loss: 655.816   log_lengthscale: -3.356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 153/20 - Loss: 661.100   log_lengthscale: -3.358\n",
      "Iter 154/20 - Loss: 689.161   log_lengthscale: -3.360\n",
      "Iter 155/20 - Loss: 595.920   log_lengthscale: -3.362\n",
      "Iter 156/20 - Loss: 513.628   log_lengthscale: -3.364\n",
      "Iter 157/20 - Loss: 759.101   log_lengthscale: -3.366\n",
      "Iter 158/20 - Loss: 714.257   log_lengthscale: -3.368\n",
      "Iter 159/20 - Loss: 683.427   log_lengthscale: -3.370\n",
      "Iter 160/20 - Loss: 594.307   log_lengthscale: -3.371\n",
      "Iter 161/20 - Loss: 678.987   log_lengthscale: -3.373\n",
      "Iter 162/20 - Loss: 580.880   log_lengthscale: -3.375\n",
      "Iter 163/20 - Loss: 630.834   log_lengthscale: -3.377\n",
      "Iter 164/20 - Loss: 721.746   log_lengthscale: -3.378\n",
      "Iter 165/20 - Loss: 600.379   log_lengthscale: -3.380\n",
      "Iter 166/20 - Loss: 632.869   log_lengthscale: -3.381\n",
      "Iter 167/20 - Loss: 625.787   log_lengthscale: -3.383\n",
      "Iter 168/20 - Loss: 622.265   log_lengthscale: -3.384\n",
      "Iter 169/20 - Loss: 586.854   log_lengthscale: -3.385\n",
      "Iter 170/20 - Loss: 526.534   log_lengthscale: -3.387\n",
      "Iter 171/20 - Loss: 641.306   log_lengthscale: -3.388\n",
      "Iter 172/20 - Loss: 536.441   log_lengthscale: -3.390\n",
      "Iter 173/20 - Loss: 518.493   log_lengthscale: -3.391\n",
      "Iter 174/20 - Loss: 642.646   log_lengthscale: -3.393\n",
      "Iter 175/20 - Loss: 583.987   log_lengthscale: -3.394\n",
      "Iter 176/20 - Loss: 477.535   log_lengthscale: -3.395\n",
      "Iter 177/20 - Loss: 567.539   log_lengthscale: -3.397\n",
      "Iter 178/20 - Loss: 523.206   log_lengthscale: -3.398\n",
      "Iter 179/20 - Loss: 662.128   log_lengthscale: -3.399\n",
      "Iter 180/20 - Loss: 574.230   log_lengthscale: -3.401\n",
      "Iter 181/20 - Loss: 713.780   log_lengthscale: -3.402\n",
      "Iter 182/20 - Loss: 600.045   log_lengthscale: -3.403\n",
      "Iter 183/20 - Loss: 608.660   log_lengthscale: -3.405\n",
      "Iter 184/20 - Loss: 640.066   log_lengthscale: -3.406\n",
      "Iter 185/20 - Loss: 562.346   log_lengthscale: -3.407\n",
      "Iter 186/20 - Loss: 683.493   log_lengthscale: -3.408\n",
      "Iter 187/20 - Loss: 584.272   log_lengthscale: -3.410\n",
      "Iter 188/20 - Loss: 546.656   log_lengthscale: -3.411\n",
      "Iter 189/20 - Loss: 612.962   log_lengthscale: -3.412\n",
      "Iter 190/20 - Loss: 520.735   log_lengthscale: -3.414\n",
      "Iter 191/20 - Loss: 492.642   log_lengthscale: -3.415\n",
      "Iter 192/20 - Loss: 789.018   log_lengthscale: -3.416\n",
      "Iter 193/20 - Loss: 529.845   log_lengthscale: -3.418\n",
      "Iter 194/20 - Loss: 640.138   log_lengthscale: -3.419\n",
      "Iter 195/20 - Loss: 631.297   log_lengthscale: -3.421\n",
      "Iter 196/20 - Loss: 545.321   log_lengthscale: -3.422\n",
      "Iter 197/20 - Loss: 594.362   log_lengthscale: -3.423\n",
      "Iter 198/20 - Loss: 461.094   log_lengthscale: -3.425\n",
      "Iter 199/20 - Loss: 555.273   log_lengthscale: -3.426\n",
      "Iter 200/20 - Loss: 569.365   log_lengthscale: -3.427\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_VariationalGPPosterior (\n",
       "  (likelihood): BernoulliLikelihood (\n",
       "  )\n",
       "  (prior_model): GPClassificationModel (\n",
       "    (likelihood): BernoulliLikelihood (\n",
       "    )\n",
       "    (mean_module): ConstantMean (\n",
       "    )\n",
       "    (covar_module): RBFKernel (\n",
       "    )\n",
       "    (grid_covar_module): GridInterpolationKernel (\n",
       "      (base_kernel_module): RBFKernel (\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gpytorch.inference import Inference\n",
    "infer = Inference(prior_model)\n",
    "posterior_model = infer.run(train_x, train_y)\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "posterior_model.train()\n",
    "optimizer = optim.Adam(posterior_model.parameters(), lr=0.1)\n",
    "optimizer.n_iter = 0\n",
    "for i in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    output = posterior_model.forward(train_x)\n",
    "    loss = -posterior_model.marginal_log_likelihood(output, train_y)\n",
    "    loss.backward()\n",
    "    optimizer.n_iter += 1\n",
    "    print('Iter %d/20 - Loss: %.3f   log_lengthscale: %.3f' % (\n",
    "        i + 1, loss.data[0],\n",
    "        posterior_model.prior_model.covar_module.log_lengthscale.data.squeeze()[0],\n",
    "    ))\n",
    "    optimizer.step()\n",
    "    \n",
    "# Set back to eval mode\n",
    "posterior_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "    1     1     1  ...     -1    -1    -1\n",
      "    1     1     1  ...     -1    -1    -1\n",
      "    1     1     1  ...     -1    -1    -1\n",
      "       ...          â‹±          ...       \n",
      "   -1    -1    -1  ...      1     1     1\n",
      "   -1    -1    -1  ...      1     1     1\n",
      "   -1    -1    -1  ...      1     1     1\n",
      "[torch.FloatTensor of size 100x100]\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARYAAADSCAYAAACPQ+9ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKwpP+8Bf+kEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11013b250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "f = plot_model_and_predictions(posterior_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
